{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c49782",
   "metadata": {},
   "source": [
    "### Bénin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL de la page principale\n",
    "url = \"https://www.globenin.com/Benin/entreprises\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Vérifier si la requête a réussi (code de statut 200 signifie succès)\n",
    "if response.status_code == 200:\n",
    "    # Analyser le contenu HTML de la page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Sélectionner la div avec la classe \"CountryEntityList\"\n",
    "    div_country_entity_list = soup.find('div', class_='CountryEntityList')\n",
    "\n",
    "    # Sélectionner tous les liens <a> à l'intérieur de la div et stocker leurs 'href' dans une liste\n",
    "    liens = div_country_entity_list.find_all('a')\n",
    "    liste_des_liens = [lien['href'] for lien in liens]\n",
    "\n",
    "    # Préfixe de l'URL\n",
    "    prefixe = \"https://www.globenin.com/\"\n",
    "\n",
    "    # Liste pour stocker les liens complets\n",
    "    liens_complets = []\n",
    "\n",
    "    # Parcourir la liste des liens et les modifier pour les rendre complets\n",
    "    for lien in liste_des_liens:\n",
    "        lien_complet = prefixe + lien.replace(\"../\", \"\")\n",
    "        liens_complets.append(lien_complet)\n",
    "    print(liens_complets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e81568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "liste_des_tous_liens=[]\n",
    "for lien in liens_complets:\n",
    "    page = 1\n",
    "    while True:\n",
    "        url = f\"{lien}/p{page}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                # Sélectionner toutes les divs avec la classe \"listentity\"\n",
    "                divs_listentity = soup.find_all('div', class_='listentity')\n",
    "               \n",
    "                # Parcourez les divs pour extraire les liens\n",
    "                for div_listentity in divs_listentity:\n",
    "                    liens = div_listentity.find_all('a')\n",
    "                    liste_des_liens = [lien['href'] for lien in liens]\n",
    "                    # Ajoutez les liens à la liste des tous les liens\n",
    "                    liste_des_tous_liens.extend(liste_des_liens)\n",
    "                    for lien in liste_des_liens:\n",
    "                        print(lien)\n",
    "\n",
    "                next_page_link = soup.find('a', class_='pagination-next')\n",
    "                if next_page_link:\n",
    "                    page += 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"Erreur lors de la récupération de la page {url}\")\n",
    "                break  # Sort de la boucle en cas d'erreur HTTP\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la récupération de la page {url}: {str(e)}\")\n",
    "            break  # Sort de la boucle en cas d'exception\n",
    "\n",
    "# Assurez-vous d'ajuster les sélecteurs CSS et les actions spécifiques à votre cas d'utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36fb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Fonction pour extraire les informations à partir d'une URL\n",
    "def extraire_informations(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Analyser le code HTML avec BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Initialiser une liste pour stocker les informations extraites de chaque div\n",
    "        data = []\n",
    "\n",
    "        # Sélectionner tous les div avec l'id \"eHold\"\n",
    "        divs_eHold = soup.find_all('div', id='eHold')\n",
    "\n",
    "        # Parcourir tous les divs eHold et extraire les informations\n",
    "        for div_eHold in divs_eHold:\n",
    "            # Extraire le nom de l'organisation (h1)\n",
    "            nom_organisation_element = div_eHold.find('h1')\n",
    "            nom_organisation = nom_organisation_element.text.strip() if nom_organisation_element else \"Non disponible\"\n",
    "\n",
    "            # Extraire la catégorie (h2)\n",
    "            categorie_element = div_eHold.find('h2')\n",
    "            categorie = categorie_element.text.strip() if categorie_element else \"Non disponible\"\n",
    "\n",
    "            # Extraire le statut juridique depuis la première balise p dans la div eHeadPres\n",
    "            statut_juridique_element = div_eHold.select_one('div#eHeadPres p:nth-of-type(1)')\n",
    "            statut_juridique = statut_juridique_element.text.strip() if statut_juridique_element else \"Non disponible\"\n",
    "\n",
    "            # Extraire le secteur d'activité depuis la deuxième balise p dans la div eHeadPres\n",
    "            secteur_activite_element = div_eHold.select_one('div#eHeadPres p:nth-of-type(2)')\n",
    "            secteur_activite = secteur_activite_element.text.strip() if secteur_activite_element else \"Non disponible\"\n",
    "\n",
    "            # Extraire tous les numéros de téléphone dans les span de classe \"spvar\" à l'intérieur de la div \"eBody\"\n",
    "            numeros_telephone_elements = div_eHold.select('div#eBody span.spvar')\n",
    "            numeros_telephone = [span.text.strip() for span in numeros_telephone_elements if '+' in span.text]\n",
    "            \n",
    "            # Extraire l'adresse email\n",
    "            adresse_email_element = div_eHold.find('a', href=True, text=re.compile(r'[\\w\\.-]+@[\\w\\.-]+'))\n",
    "            adresse_email = adresse_email_element['href'] if adresse_email_element else \"Non disponible\"\n",
    "\n",
    "            # Extraire le site web\n",
    "            site_web_element = div_eHold.find('a', href=True, text=re.compile(r'^https?://'))\n",
    "            site_web = site_web_element['href'] if site_web_element else \"Non disponible\"\n",
    "            \n",
    "            contenu_spvar_texte = \"Non disponible\"\n",
    "            # Parcourir tous les éléments <p> à l'intérieur de div_eHold\n",
    "            for paragraphe_element in div_eHold.find_all('p'):\n",
    "                strong_element = paragraphe_element.find('strong')\n",
    "                # Vérifier si l'élément <strong> existe dans le paragraphe\n",
    "                if strong_element:\n",
    "                    spans_spvar = strong_element.find_all('span', class_='spvar')\n",
    "                    # Extraire le contenu de tous les <span> avec la classe \"spvar\"\n",
    "                    contenu_spvar = [span.text.strip() for span in spans_spvar]\n",
    "                    contenu_spvar_texte = ', '.join(contenu_spvar)\n",
    "                    # Sortir de la boucle une fois que le contenu a été trouvé\n",
    "                    break\n",
    "                    \n",
    "            localisation = \"Non disponible\"\n",
    "            ville = \"Non disponible\"\n",
    "            # Parcourir tous les éléments <p> à l'intérieur de div_eHold\n",
    "            for paragraphe_element in div_eHold.find_all('p'):\n",
    "                splab_element = paragraphe_element.find('span', class_='splab')\n",
    "                fas_city_element = paragraphe_element.find('span', class_='fas fa-city')\n",
    "                fas_map_marker_element = paragraphe_element.find('span', class_='fas fa-map-marker-alt')\n",
    "\n",
    "                # Vérifier si l'élément <span class=\"splab\"> existe dans le paragraphe\n",
    "                if splab_element:\n",
    "                    spvar_element = paragraphe_element.find('span', class_='spvar')\n",
    "\n",
    "                    # Vérifier si l'élément <span class=\"spvar\"> existe\n",
    "                    if spvar_element:\n",
    "                        texte_spvar = spvar_element.text.strip()\n",
    "\n",
    "                        # Vérifier si l'élément <span class=\"fas fa-map-marker-alt\"> existe\n",
    "                        if fas_map_marker_element:\n",
    "                            localisation = texte_spvar\n",
    "                        # Vérifier si l'élément <span class=\"fas fa-city\"> existe\n",
    "                        elif fas_city_element:\n",
    "                            ville = texte_spvar\n",
    "            # Extraire la référence\n",
    "            reference_element = div_eHold.find('span', text='Réf.')\n",
    "            reference = reference_element.find_next('span').text.strip() if reference_element else \"Non disponible\"\n",
    "\n",
    "            # Ajouter les informations extraites dans la liste data\n",
    "            data.append({\n",
    "                \"Nom de l'organisation\": nom_organisation,\n",
    "                \"Catégorie\": categorie,\n",
    "                \"Statut juridique\": statut_juridique,\n",
    "                \"Secteur d'activité\": secteur_activite,\n",
    "                \"Numéros de téléphone\": ', '.join(numeros_telephone),\n",
    "                \"Adresse email\": adresse_email,\n",
    "                \"Site web\": site_web,\n",
    "                \"Référence\": reference,\n",
    "                \"Nom de la personne\": contenu_spvar_texte,\n",
    "                \"Localisation\": localisation,  # Ajout de la localisation\n",
    "                \"Ville\": ville  # Ajout de la ville\n",
    "            })\n",
    "\n",
    "        # Créer un DataFrame à partir de la liste de données\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Retourner le DataFrame\n",
    "        return df\n",
    "\n",
    "    else:\n",
    "        print(f\"Échec de la requête pour l'URL {url}. Code de statut : {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    \n",
    "# Créer une liste de DataFrames pour chaque URL\n",
    "liste_de_dataframes = []\n",
    "\n",
    "# Parcourir la liste des URL et extraire les informations pour chaque URL\n",
    "for url in liste_des_tous_liens:\n",
    "    df = extraire_informations(url)\n",
    "    if df is not None:\n",
    "        liste_de_dataframes.append(df)\n",
    "\n",
    "# Concaténer tous les DataFrames en un seul DataFrame\n",
    "dataframe_final = pd.concat(liste_de_dataframes, ignore_index=True)\n",
    "\n",
    "# Afficher le DataFrame final\n",
    "dataframe_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d32bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file_path = \"GL_BENIN.xlsx\"\n",
    "\n",
    "# Enregistrez le DataFrame dans le classeur Excel\n",
    "dataframe_final.to_excel(excel_file_path, index=False)\n",
    "\n",
    "# Confirmez l'enregistrement\n",
    "print(f\"Le DataFrame a été enregistré dans {excel_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced6154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chemin_fichier_excel = \"GL_BENIN.xlsx\"\n",
    "\n",
    "df = pd.read_excel(chemin_fichier_excel)\n",
    "\n",
    "# Importer pandas\n",
    "import pandas as pd\n",
    "# Remplacer les valeurs vides par 'Non disponible'\n",
    "df.fillna('Non disponible', inplace=True)\n",
    "\n",
    "# Nettoyer la colonne \"Statut juridique\"\n",
    "df['Statut juridique'] = df['Statut juridique'].str.replace('Statut juridique:', '').str.strip()\n",
    "\n",
    "# Nettoyer la colonne \"Secteur d'activité\"\n",
    "df['Secteur d\\'activité'] = df['Secteur d\\'activité'].str.replace('Secteur d\\'activité:', '').str.strip()\n",
    "\n",
    "# Diviser la colonne \"Numéros de téléphone\" en une liste de numéros\n",
    "df['Numéros de téléphone'] = df['Numéros de téléphone'].str.split(',')\n",
    "\n",
    "# Créer des colonnes pour les numéros de téléphone, avec des valeurs par défaut \"Non disponible\"\n",
    "df['Téléphone 1'] = 'Non disponible'\n",
    "df['Téléphone 2'] = 'Non disponible'\n",
    "df['Téléphone 3'] = 'Non disponible'\n",
    "\n",
    "# Remplir les colonnes de téléphone avec les numéros disponibles\n",
    "df['Téléphone 1'] = df['Numéros de téléphone'].apply(lambda x: x[0] if len(x) > 0 else 'Non disponible')\n",
    "df['Téléphone 2'] = df['Numéros de téléphone'].apply(lambda x: x[1] if len(x) > 1 else 'Non disponible')\n",
    "df['Téléphone 3'] = df['Numéros de téléphone'].apply(lambda x: x[2] if len(x) > 2 else 'Non disponible')\n",
    "\n",
    "# Diviser la colonne \"Nom de la personne\" en deux colonnes distinctes (Nom et Poste)\n",
    "df[['Nom de la personne', 'Poste']] = df['Nom de la personne'].str.split('/', expand=True)\n",
    "\n",
    "# Nettoyer la colonne \"Adresse email\"\n",
    "df['Adresse email'] = df['Adresse email'].str.replace('mailto:', '').str.strip()\n",
    "\n",
    "df.drop(columns=['Numéros de téléphone'], inplace=True)\n",
    "# Afficher le DataFrame nettoyé\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
